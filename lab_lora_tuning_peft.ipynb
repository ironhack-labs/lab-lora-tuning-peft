{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/solvedbrunus/lab-lora-tuning-peft/blob/main/lab_lora_tuning_peft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9Y40X0WCY_H"
      },
      "source": [
        "# Lab | Introduction to LoRA Tuning using PEFT from Hugging Face\n",
        "<!-- ### Fine-tune a Foundational Model effortlessly -->\n",
        "\n",
        "**Note:** This is more or less the same notebook you saw in the previous lesson, but that is ok. This is an LLM fine-tuning lab. In class we used a set of datasets and models, and in the labs you are required to change the LLMs models and the datasets including the pre-processing pipelines.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCinY2l-upqy"
      },
      "source": [
        "# LoRA Tuning\n",
        "\n",
        "In this notebook you are being introduced to how to apply LoRA Tuning with the PEFT library to a pre-trained model.\n",
        "\n",
        "For a complete list of Models compatible with PEFT refer to their [documentation](https://huggingface.co/docs/peft/main/en/index#supported-methods).\n",
        "\n",
        "A short sample of models families available to be trained with PEFT are: Bloom, Llama, GPT-J, GPT-2, BERT... and more. Hugging Face is working hard to bring more Models to the Library.\n",
        "\n",
        "## Brief introduction to LoRA Tuning.\n",
        "LoRA is a re-parameterization technique. Its operation is simple, complex, and brilliant at the same time. It involves reducing the size of the matrices to be trained by dividing them in such a way that when multiplied, they yield the original matrix.\n",
        "\n",
        "The weights that are modified are those of the reduced matrices, not the original matrix. It's better visualized in an image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp_7rR4Nx7My"
      },
      "source": [
        "![Matrices_LoRA.webp](data:image/webp;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oSLJkXty-t_"
      },
      "source": [
        "We have an original matrix of 50x50, which means we would have to modify about 2500 parameters. However, as we know, if we multiply two matrices of (2x50) and (50x2), we obtain a 50x50 matrix. Yet, these two matrices are formed by only 100 parameters each. In other words, for the reduced matrices, we need to modify a total of 200 parameters compared to the 2500 of the original matrix. This represents a 92% reduction, and the larger the original matrix, the greater the percentage of savings.\n",
        "\n",
        "In Language Models like GPT-3 or any of the current ones with LoRA, it's possible that we only need to train about 0.02% of the original parameters. This varies for each model. The best part is that the obtained result is very similar to that of full fine-tuning, in some cases, it can even be better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "620MdVMk7iUS"
      },
      "source": [
        "# Load the PEFT and Datasets Libraries.\n",
        "\n",
        "The PEFT library contains the Hugging Face implementation of differente fine-tuning techniques, like LoRA Tuning.\n",
        "\n",
        "Using the Datasets library we have acces to a huge amount of Datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UyyuMGnCPjA",
        "outputId": "4b52ac24-5f7a-44c4-a46c-f72b65625ec6",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/507.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/115.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/166.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/135.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/194.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting ipywidgets==7.7.5\n",
            "  Downloading ipywidgets-7.7.5-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets==7.7.5) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets==7.7.5) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets==7.7.5) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.4 in /usr/local/lib/python3.11/dist-packages (from ipywidgets==7.7.5) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets==7.7.5) (7.34.0)\n",
            "Collecting jupyterlab-widgets<3,>=1.0.0 (from ipywidgets==7.7.5)\n",
            "  Downloading jupyterlab_widgets-1.1.11-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.7.5) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.7.5) (6.3.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.5) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets==7.7.5)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.5) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.5) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.5) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.5) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.5) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.5) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.5) (4.9.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (6.5.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets==7.7.5) (0.8.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (3.1.5)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (5.7.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (7.16.5)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets==7.7.5) (2.8.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets==7.7.5) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets==7.7.5) (0.2.13)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (4.3.6)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (4.12.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (3.1.0)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (0.10.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (24.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (4.23.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->jupyter-client->ipykernel>=4.5.1->ipywidgets==7.7.5) (1.17.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (1.4.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (0.22.3)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (2.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (1.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (1.3.1)\n",
            "Downloading ipywidgets-7.7.5-py2.py3-none-any.whl (123 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.9/123.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_widgets-1.1.11-py3-none-any.whl (246 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.9/246.9 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jupyterlab-widgets, jedi, ipywidgets\n",
            "  Attempting uninstall: jupyterlab-widgets\n",
            "    Found existing installation: jupyterlab_widgets 3.0.13\n",
            "    Uninstalling jupyterlab_widgets-3.0.13:\n",
            "      Successfully uninstalled jupyterlab_widgets-3.0.13\n",
            "  Attempting uninstall: ipywidgets\n",
            "    Found existing installation: ipywidgets 7.7.1\n",
            "    Uninstalling ipywidgets-7.7.1:\n",
            "      Successfully uninstalled ipywidgets-7.7.1\n",
            "Successfully installed ipywidgets-7.7.5 jedi-0.19.2 jupyterlab-widgets-1.1.11\n"
          ]
        }
      ],
      "source": [
        "!pip install -q peft\n",
        "!pip install -q datasets==2.16.1\n",
        "!pip install ipywidgets==7.7.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOnJlBY-81Wl"
      },
      "source": [
        "From the transformers library we import the necesary classes to import the model and the tokenizer.\n",
        "\n",
        "Then we can load the Tokenizer and the model.\n",
        "\n",
        "Bloom is one of the smallest and smarter model available to be trained with PEFT Library using Prompt Tuning. You can use either of the models in the Bloom Family, I encorage you to use at least two of them and see the differences.\n",
        "\n",
        "I'm using the smallest one just to spend less time trainig, and avoid memory problems in Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "id": "vziwd2UuCYGl",
        "outputId": "e1975dfb-d2c0-4a9c-d59c-3220a4f3abc9",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-1B.\n401 Client Error. (Request ID: Root=1-678e829e-304af93c5dd01de77a6fa6b1;02d862e5-8826-4594-8b7f-ae407c88a850)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.2-1B is restricted. You must have access to it and be authenticated to access it. Please log in.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-3.2-1B/resolve/main/config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[0;31m# Repo not found or gated => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1483\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1373\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1375\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1293\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1295\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    422\u001b[0m             )\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGatedRepoError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-678e829e-304af93c5dd01de77a6fa6b1;02d862e5-8826-4594-8b7f-ae407c88a850)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.2-1B is restricted. You must have access to it and be authenticated to access it. Please log in.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-3813774551df>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"meta-llama/Llama-3.2-1B\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mfoundation_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    876\u001b[0m                     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m                     config = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    879\u001b[0m                         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         \u001b[0mcode_revision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"code_revision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m         \u001b[0mhas_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0mhas_local_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m                 \u001b[0;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    650\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresolved_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_raise_exceptions_for_gated_repo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresolved_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    422\u001b[0m             \u001b[0;34m\"You are trying to access a gated repo.\\nMake sure to have access to it at \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;34mf\"https://huggingface.co/{path_or_repo_id}.\\n{str(e)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-1B.\n401 Client Error. (Request ID: Root=1-678e829e-304af93c5dd01de77a6fa6b1;02d862e5-8826-4594-8b7f-ae407c88a850)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.2-1B is restricted. You must have access to it and be authenticated to access it. Please log in."
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"bigscience/bloomz-1b1\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, device_map='cuda')\n",
        "foundation_model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, device_map='cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qtc1gbK39Hp7"
      },
      "source": [
        "## Inference with the pre-trained model.\n",
        "I'm going to do a test with the pre-trained model without fine-tuning, to see if something changes after the fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "jak6FzpvFTHk",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#this function returns the outputs from the model received, and inputs.\n",
        "def get_outputs(model, inputs, max_new_tokens=1000): #play with this function inputs and see if you get something interesting\n",
        "    outputs = foundation_model.generate(\n",
        "            input_ids=input_sentences[\"input_ids\"],\n",
        "            attention_mask=input_sentences[\"attention_mask\"],\n",
        "            max_length=100,\n",
        "            num_beams=5,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.5,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            do_sample=True,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkFqjS459jAa"
      },
      "source": [
        "The dataset used for the fine-tuning contains prompts to be used with Large Language Models.\n",
        "\n",
        "I'm going to request the pre-trained model that acts like a motivational coach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BAYg7czFYeK",
        "outputId": "88f76adf-8f88-4e0f-c03a-efc6c0033f44",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"I want you to act as a motivational coach. Don't be afraid of being challenged. Don't be afraid of being criticized. Don't be afraid of being wrong. Don't be afraid of being alone. Don't be afraid of being scared. Don't be afraid of being lonely. Don't be afraid of being alone. Don't be afraid of being alone. Don't be afraid of being alone. Don't be afraid of being alone. Don't be afraid of being alone. Don't be afraid of being alone. Don't be afraid of being\"]\n"
          ]
        }
      ],
      "source": [
        "#Inference original model\n",
        "input_sentences = tokenizer(\"I want you to act as a motivational coach. Don't be afraid of being challenged\", return_tensors=\"pt\", padding=True)\n",
        "input_sentences = {k: v.to('cuda') for k, v in input_sentences.items()}\n",
        "foundational_outputs_sentence = get_outputs(foundation_model, input_sentences, max_new_tokens=1000)\n",
        "\n",
        "print(tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQUGY47p9ysI"
      },
      "source": [
        "Not sure if the answer is correct or not, but for sure is not a prompt. We need to train our model if we want that acts like a prompt engineer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL5L_DcR9ggA"
      },
      "source": [
        "# Preparing the Dataset.\n",
        "The Dataset used is:\n",
        "\n",
        "https://huggingface.co/datasets/fka/awesome-chatgpt-prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFjA4ONDsOq3",
        "outputId": "514c891e-5efa-4957-bf30-ec2792577f59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.16.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.11/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269,
          "referenced_widgets": [
            "5aaaa31fddf04ede864fc9b19a2fbeaa",
            "493ec0e28f8d48d0a3126c20acaa9dca",
            "916199f9a6d0479ea2025be269a471a1",
            "099bad9fbdb44f8aa1c4ca64b6ec0456",
            "937cbccbac714c6e9cdf3c9a15d2e712",
            "cb538a3c2a86491bac86d547c1a4163c",
            "313e5b5b3e97413c861463eb97380746",
            "78a69038d2694cfe9b620eaec20af1c7",
            "2b8d692a4b8243fd9e5b4dedb5d2509b",
            "cfbee8c3b6a840bc9ad897d53bc6d9fc",
            "2f8ab9e083a44301a804cc927cc6d5e5",
            "8bdfb3b7d23e4087bd4ac984a43b5866",
            "3f54df057eee4856ad9e1d149264d5b9",
            "f3f4c1287f824ab7bd3b9d342f21f55a",
            "6f1b6e02806047129c7cda7864df0459",
            "4c3031e0f7aa4f91a1229581ca6442dc",
            "38a457901c2044c899eb5d21ef6458c3",
            "f5f50986beca4a2b9d92dcff8780191f",
            "3556679362a448129ed3e2ea0a75ca10",
            "bb48da502d0245c4a4307caee59914a4",
            "8ce3806fbede47e49c4a3a9f588e46c8",
            "7e7102b7cddf4ae5990d229f3646a83e",
            "90b687ae4b654bcba8aa6e8724adfd19",
            "79a6ff418b984a8888a49c9f5addebd0",
            "3e1f5490f43f451a910fdb23ba209814",
            "fa7bd41fd6774a988dd1686e111fa244",
            "b05d2235cd5d4ef78f3fbc2848696c4c",
            "8c8cbbe88f67408dbec1afb227e6401e",
            "a546b3c7996845aaa74f66133f5a142a",
            "a907399bb2d146d0b5901914fbb4e105",
            "6a2b9a075d4946499657cfabdc844993",
            "9841b682cf5f47818746326eef77dc8c",
            "86a7a0062a3d4b15819f7e4d593824be",
            "e77e61c974734639abaa4a323f3d3454",
            "54d6445d0ab54317adcc74cf4cefaf7d",
            "5aee7c05acc746fe966f350299d1c5da",
            "5cb3cb061b6f4889b0f2cd0fde1b0fbd",
            "ccf904a9370a46aea8842989f75c36d4",
            "c5007086d59e4b3ab0dbc085b14c27c0",
            "a906dfd77a174e4ebe758ca96ff0fc16",
            "62ebded176d742a1977b14aa54baef57",
            "ce109a82f3c74648914b28afc5840e20",
            "292ff922522e43a88c8309d35354464e",
            "73ead991522740389e7e0a2a422e0a88"
          ]
        },
        "id": "DyIMQ7IHFbIx",
        "outputId": "115b8923-9259-43a4-b862-970785643364",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/339 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5aaaa31fddf04ede864fc9b19a2fbeaa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/104k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8bdfb3b7d23e4087bd4ac984a43b5866"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "90b687ae4b654bcba8aa6e8724adfd19"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/datasets/download/streaming_download_manager.py:778: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n",
            "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/203 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e77e61c974734639abaa4a323f3d3454"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['prompt', 'input_ids', 'attention_mask'],\n",
              "    num_rows: 50\n",
              "})"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Correct the dataset name (repo name, not URL)\n",
        "dataset = \"fka/awesome-chatgpt-prompts\"\n",
        "\n",
        "# Load dataset directly from HuggingFace\n",
        "data = load_dataset(dataset)\n",
        "\n",
        "# Process the dataset\n",
        "data = data.map(lambda samples: tokenizer(samples['prompt']), batched=True)\n",
        "train_sample = data['train'].select(range(50))\n",
        "\n",
        "# Remove unwanted columns\n",
        "train_sample = train_sample.remove_columns('act')\n",
        "display(train_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmlZY3fk_9fm",
        "outputId": "fc50301a-bce5-4e95-c3a6-974d58631be4",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'prompt': ['Imagine you are an experienced Ethereum developer tasked with creating a smart contract for a blockchain messenger. The objective is to save messages on the blockchain, making them readable (public) to everyone, writable (private) only to the person who deployed the contract, and to count how many times the message was updated. Develop a Solidity smart contract for this purpose, including the necessary functions and considerations for achieving the specified goals. Please provide the code and any relevant explanations to ensure a clear understanding of the implementation.'], 'input_ids': [[186402, 1152, 1306, 660, 72560, 28857, 167625, 84544, 20165, 376, 1002, 26168, 267, 30479, 17477, 613, 267, 120755, 238776, 17, 1387, 47881, 632, 427, 14565, 29866, 664, 368, 120755, 15, 16997, 4054, 136044, 375, 4859, 12, 427, 39839, 15, 9697, 1242, 375, 13614, 12, 3804, 427, 368, 2298, 5268, 109891, 368, 17477, 15, 530, 427, 11210, 4143, 7112, 11866, 368, 11011, 1620, 36320, 17, 21265, 267, 11550, 90533, 30479, 17477, 613, 1119, 27343, 15, 11762, 368, 18348, 16231, 530, 127246, 613, 94510, 368, 25605, 55790, 17, 29901, 13842, 368, 4400, 530, 2914, 24466, 184637, 427, 22646, 267, 11285, 32391, 461, 368, 17786, 17]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
          ]
        }
      ],
      "source": [
        "print(train_sample[:1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVPAJsrUAHiJ"
      },
      "source": [
        "# Fine-Tuning.\n",
        "First is necesary create a LoRA config.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uCalslQFGL7K"
      },
      "outputs": [],
      "source": [
        "# TARGET_MODULES\n",
        "# https://github.com/huggingface/peft/blob/39ef2546d5d9b8f5f8a7016ec10657887a867041/src/peft/utils/other.py#L220\n",
        "\n",
        "import peft\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "\n",
        "lora_config = LoraConfig(#play with these config inputs\n",
        "    r=4, #As bigger the R bigger the parameters to train.\n",
        "    lora_alpha=1, # a scaling factor that adjusts the magnitude of the weight matrix. Usually set to 1\n",
        "    target_modules=[\"query_key_value\"], #You can obtain a list of target modules in the URL above.\n",
        "    lora_dropout=0.05, #Helps to avoid Overfitting.\n",
        "    bias=\"lora_only\", # this specifies if the bias parameter should be trained.\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUddynl0B1Ck"
      },
      "source": [
        "The most important parameter is **r**, it defines how many parameters will be trained. As bigger the valuer more parameters are trained, but it means that the model will be able to learn more complicated relations between input and output.\n",
        "\n",
        "Yo can find a list of the **target_modules** available on the [Hugging Face Documentation]( https://github.com/huggingface/peft/blob/39ef2546d5d9b8f5f8a7016ec10657887a867041/src/peft/utils/other.py#L220)\n",
        "\n",
        "**lora_dropout** is like the commom dropout is used to avoid overfitting.\n",
        "\n",
        "**bias** I was hesitating if use *none* or *lora_only*. For text classification the most common value is none, and for chat or question answering, *all* or *lora_only*.\n",
        "\n",
        "**task_type**. Indicates the task the model is beign trained for. In this case, text generation.\n",
        "\n",
        "### Create the PEFT model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BG1zBmhsGQ-h",
        "outputId": "28a4f762-e6e4-4951-9875-c2bc875302b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 700,416 || all params: 1,065,904,128 || trainable%: 0.0657\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "peft_model = get_peft_model(foundation_model, lora_config)\n",
        "print(peft_model.print_trainable_parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2onXUsaw-Ga0"
      },
      "source": [
        "The number of trainable parameters is really small compared with the total number of parameters in the pre-trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HArPQ_lvGUkY"
      },
      "outputs": [],
      "source": [
        "#Create a directory to contain the Model\n",
        "import os\n",
        "working_dir = './'\n",
        "\n",
        "output_directory = os.path.join(working_dir, \"peft_lab_outputs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWalmqWm4STo"
      },
      "source": [
        "In the TrainingArgs we inform the number of epochs we want to train, the output directory and the learning_rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ND0aJ-t6ARqD"
      },
      "outputs": [],
      "source": [
        "#Creating the TrainingArgs\n",
        "import transformers\n",
        "from transformers import TrainingArguments, Trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_directory,\n",
        "    auto_find_batch_size=True, # Find a correct bvatch size that fits the size of Data.\n",
        "    learning_rate= 3e-2, # Higher learning rate than full fine-tuning.\n",
        "    num_train_epochs=2,\n",
        "    use_cpu=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgxsV-iy_J_o"
      },
      "source": [
        "Now we can train the model.\n",
        "To train the model we need:\n",
        "\n",
        "\n",
        "*   The PEFT Model.\n",
        "*   The training_args\n",
        "* The Dataset\n",
        "* The result of DataCollator, the Dataset ready to be procesed in blocks.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "z5NYHqBnGZyF"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_sample,\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(\n",
        "        tokenizer,\n",
        "        mlm=False,\n",
        "        # Add these parameters to handle batch size\n",
        "        pad_to_multiple_of=8,  # Helps with memory efficiency\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "e5dMiNqep31B"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=4,  # Reduce this if still getting memory errors\n",
        "    gradient_accumulation_steps=4,   # Increase this to compensate for smaller batch size\n",
        "    max_grad_norm=0.3,              # Add gradient clipping\n",
        "    learning_rate=2e-4,\n",
        "    max_steps=100,                  # Adjust as needed\n",
        "    save_total_limit=3,\n",
        "    logging_steps=10,\n",
        "    output_dir=\"./output\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Wq9-3OJ8p31B"
      },
      "outputs": [],
      "source": [
        "peft_model.gradient_checkpointing_enable()  # Add this before creating the Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "kEKiFdpDGgOx"
      },
      "outputs": [],
      "source": [
        "#Save the model.\n",
        "peft_model_path = os.path.join(output_directory, f\"lora_model\")\n",
        "\n",
        "trainer.model.save_pretrained(peft_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_TAjrSWSe14q"
      },
      "outputs": [],
      "source": [
        "#Load the Model.\n",
        "loaded_model = PeftModel.from_pretrained(foundation_model,\n",
        "                                        peft_model_path,\n",
        "                                        is_trainable=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK--YFPR6OxH"
      },
      "source": [
        "## Inference the fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_27uvJudf03",
        "outputId": "47ebd3dc-6685-46b6-8e7a-22ae946475df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['You are a motivational coach']\n"
          ]
        }
      ],
      "source": [
        "input_sentences = tokenizer(\"You are a motivational coach\", return_tensors=\"pt\")\n",
        "input_sentences = {k: v.to('cuda') for k, v in input_sentences.items()}\n",
        "foundational_outputs_sentence = get_outputs(loaded_model, input_sentences, max_new_tokens=500)\n",
        "\n",
        "print(tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCV9JOBG6Ug8"
      },
      "source": [
        "The result is amazing. Let's compare the answer of the pre-trained Model withe the one fine-tuned by us using LoRA:\n",
        "* **Pretrained Model:** *I want you to act as a motivational coach.*  Don't be afraid of being challenged.\n",
        "* **Fine-Tuned Model:** I want you to act as a motivational coach.  I will provide some information about someone\\'s motivation and goals, but it should be your job  in order my first request – \"I need someone who can help me find the best way for myself stay motivated when competing against others.\" My suggestion is “I have\n",
        "\n",
        "As you can see the result is really similar to the samples containmed in the Datased used to fine-tune the Model. And we only trained the Model for 10 epochs and with a really small number of rows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tr4Sm32y89Ji"
      },
      "source": [
        "# Exercise\n",
        "\n",
        "- Drive your own experiments with all the variables and different model types.\n",
        "    - Please with the **lora_config** values, maybe you can achieve a better result in less epochs, saving time and money for your company. :-)\n",
        "- Write a one page report\n",
        "    - What did you learn?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "saAL3Jn2qolX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "outputId": "8b96178d-35dd-4828-c916-652170ab2037"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct.\n401 Client Error. (Request ID: Root=1-678e8340-4e2a5e5c6a248a333e33904c;7d9d5093-6905-41cc-ac6f-f113491365ca)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.3-70B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/resolve/main/config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[0;31m# Repo not found or gated => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1483\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1373\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1375\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1293\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1295\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    422\u001b[0m             )\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGatedRepoError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-678e8340-4e2a5e5c6a248a333e33904c;7d9d5093-6905-41cc-ac6f-f113491365ca)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.3-70B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-93e88729619a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"meta-llama/Llama-3.3-70B-Instruct\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mfoundation_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    876\u001b[0m                     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m                     config = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    879\u001b[0m                         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         \u001b[0mcode_revision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"code_revision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m         \u001b[0mhas_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0mhas_local_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m                 \u001b[0;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    650\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresolved_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_raise_exceptions_for_gated_repo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresolved_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    422\u001b[0m             \u001b[0;34m\"You are trying to access a gated repo.\\nMake sure to have access to it at \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;34mf\"https://huggingface.co/{path_or_repo_id}.\\n{str(e)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct.\n401 Client Error. (Request ID: Root=1-678e8340-4e2a5e5c6a248a333e33904c;7d9d5093-6905-41cc-ac6f-f113491365ca)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.3-70B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in."
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "foundation_model = AutoModelForCausalLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "WxW8NUOIp31D"
      },
      "outputs": [],
      "source": [
        "#this function returns the outputs from the model received, and inputs.\n",
        "def get_outputs(model, inputs, max_new_tokens=100): #play with this function inputs and see if you get something interesting\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        repetition_penalty=1.5, #Avoid repetition.\n",
        "        early_stopping=True, #The model can stop before reach the max_length\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "t_y37LBqp31D",
        "outputId": "04f061e4-cbf6-48a7-a093-a67bea12b599"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:676: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2134: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-14fb6374f432>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Inference original model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minput_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You are a motivati \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfoundational_outputs_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfoundation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfoundational_outputs_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-d46a36d1e8fe>\u001b[0m in \u001b[0;36mget_outputs\u001b[0;34m(model, inputs, max_new_tokens)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#this function returns the outputs from the model received, and inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#play with this function inputs and see if you get something interesting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     outputs = model.generate(\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2251\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2252\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2253\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2254\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3250\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_prefill\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3251\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3252\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3253\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bloom/modeling_bloom.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **deprecated_arguments)\u001b[0m\n\u001b[1;32m    971\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bloom/modeling_bloom.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **deprecated_arguments)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;31m# kept for BC (non `Cache` `past_key_values` inputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1844\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1845\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m             \u001b[0;31m# run always called hooks if they have not already been run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36minner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1788\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1790\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1791\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m                 for hook_id, hook in (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
          ]
        }
      ],
      "source": [
        "#Inference original model\n",
        "input_sentences = tokenizer(\"You are a motivati \", return_tensors=\"pt\")\n",
        "foundational_outputs_sentence = get_outputs(foundation_model, input_sentences, max_new_tokens=50)\n",
        "\n",
        "print(tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pE8217rip31I"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = \"fka/awesome-chatgpt-prompts\"\n",
        "\n",
        "#Create the Dataset to create prompts.\n",
        "data = load_dataset(dataset)\n",
        "data = data.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n",
        "train_sample = data[\"train\"].select(range(50))\n",
        "\n",
        "train_sample = train_sample.remove_columns('act')\n",
        "\n",
        "display(train_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xpyn5q9Np31J"
      },
      "outputs": [],
      "source": [
        "# TARGET_MODULES\n",
        "# https://github.com/huggingface/peft/blob/39ef2546d5d9b8f5f8a7016ec10657887a867041/src/peft/utils/other.py#L220\n",
        "\n",
        "import peft\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "\n",
        "lora_config = LoraConfig(#play with these config inputs\n",
        "    r=4, #As bigger the R bigger the parameters to train.\n",
        "    lora_alpha=1, # a scaling factor that adjusts the magnitude of the weight matrix. Usually set to 1\n",
        "    target_modules=[\"query_key_value\"], #You can obtain a list of target modules in the URL above.\n",
        "    lora_dropout=0.01, #Helps to avoid Overfitting.\n",
        "    bias=\"lora_only\", # this specifies if the bias parameter should be trained.\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cFz0sx3p31J"
      },
      "outputs": [],
      "source": [
        "peft_model = get_peft_model(foundation_model, lora_config)\n",
        "print(peft_model.print_trainable_parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muo1ic7Lp31J"
      },
      "outputs": [],
      "source": [
        "#Create a directory to contain the Model\n",
        "import os\n",
        "working_dir = './'\n",
        "\n",
        "output_directory = os.path.join(working_dir, \"peft_lab_outputs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLdV91sfp31J"
      },
      "outputs": [],
      "source": [
        "#Creating the TrainingArgs\n",
        "import transformers\n",
        "from transformers import TrainingArguments, Trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_directory,\n",
        "    auto_find_batch_size=8, # Find a correct bvatch size that fits the size of Data.\n",
        "    learning_rate= 5e-2, # Higher learning rate than full fine-tuning.\n",
        "    num_train_epochs=2,\n",
        "    evaluation_strategy=\"no\",\n",
        "    eval_steps=10,\n",
        "    logging_steps=10,\n",
        "    save_steps=20,\n",
        "    use_cpu=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnjqhX7Qp31J"
      },
      "outputs": [],
      "source": [
        "#This cell may take up to 15 minutes to execute.\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_sample,\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68nZRlGcp31J"
      },
      "outputs": [],
      "source": [
        "#Save the model.\n",
        "peft_model_path = os.path.join(output_directory, f\"lora_model\")\n",
        "\n",
        "trainer.model.save_pretrained(peft_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4JCgwo-p31K"
      },
      "outputs": [],
      "source": [
        "#Load the Model.\n",
        "loaded_model = PeftModel.from_pretrained(foundation_model,\n",
        "                                        peft_model_path,\n",
        "                                        is_trainable=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3U0GrF2Sp31K"
      },
      "outputs": [],
      "source": [
        "input_sentences = tokenizer(\"YOUR QUERY GOES HERE\", return_tensors=\"pt\")\n",
        "foundational_outputs_sentence = get_outputs(loaded_model, input_sentences, max_new_tokens=50)\n",
        "\n",
        "print(tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I tried to run it but i difficulties, some of the results when run in class with the teacher. There was an improvement in generating contextual motivational coaching responses."
      ],
      "metadata": {
        "id": "m5nXVFUSslij"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VuVKA-Y3tDHz"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5aaaa31fddf04ede864fc9b19a2fbeaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_493ec0e28f8d48d0a3126c20acaa9dca",
              "IPY_MODEL_916199f9a6d0479ea2025be269a471a1",
              "IPY_MODEL_099bad9fbdb44f8aa1c4ca64b6ec0456"
            ],
            "layout": "IPY_MODEL_937cbccbac714c6e9cdf3c9a15d2e712"
          }
        },
        "493ec0e28f8d48d0a3126c20acaa9dca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb538a3c2a86491bac86d547c1a4163c",
            "placeholder": "​",
            "style": "IPY_MODEL_313e5b5b3e97413c861463eb97380746",
            "value": "Downloading readme: 100%"
          }
        },
        "916199f9a6d0479ea2025be269a471a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78a69038d2694cfe9b620eaec20af1c7",
            "max": 339,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2b8d692a4b8243fd9e5b4dedb5d2509b",
            "value": 339
          }
        },
        "099bad9fbdb44f8aa1c4ca64b6ec0456": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfbee8c3b6a840bc9ad897d53bc6d9fc",
            "placeholder": "​",
            "style": "IPY_MODEL_2f8ab9e083a44301a804cc927cc6d5e5",
            "value": " 339/339 [00:00&lt;00:00, 20.1kB/s]"
          }
        },
        "937cbccbac714c6e9cdf3c9a15d2e712": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb538a3c2a86491bac86d547c1a4163c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "313e5b5b3e97413c861463eb97380746": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78a69038d2694cfe9b620eaec20af1c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b8d692a4b8243fd9e5b4dedb5d2509b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cfbee8c3b6a840bc9ad897d53bc6d9fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f8ab9e083a44301a804cc927cc6d5e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8bdfb3b7d23e4087bd4ac984a43b5866": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f54df057eee4856ad9e1d149264d5b9",
              "IPY_MODEL_f3f4c1287f824ab7bd3b9d342f21f55a",
              "IPY_MODEL_6f1b6e02806047129c7cda7864df0459"
            ],
            "layout": "IPY_MODEL_4c3031e0f7aa4f91a1229581ca6442dc"
          }
        },
        "3f54df057eee4856ad9e1d149264d5b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38a457901c2044c899eb5d21ef6458c3",
            "placeholder": "​",
            "style": "IPY_MODEL_f5f50986beca4a2b9d92dcff8780191f",
            "value": "Downloading data: 100%"
          }
        },
        "f3f4c1287f824ab7bd3b9d342f21f55a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3556679362a448129ed3e2ea0a75ca10",
            "max": 104186,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bb48da502d0245c4a4307caee59914a4",
            "value": 104186
          }
        },
        "6f1b6e02806047129c7cda7864df0459": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ce3806fbede47e49c4a3a9f588e46c8",
            "placeholder": "​",
            "style": "IPY_MODEL_7e7102b7cddf4ae5990d229f3646a83e",
            "value": " 104k/104k [00:00&lt;00:00, 414kB/s]"
          }
        },
        "4c3031e0f7aa4f91a1229581ca6442dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38a457901c2044c899eb5d21ef6458c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5f50986beca4a2b9d92dcff8780191f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3556679362a448129ed3e2ea0a75ca10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb48da502d0245c4a4307caee59914a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8ce3806fbede47e49c4a3a9f588e46c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e7102b7cddf4ae5990d229f3646a83e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "90b687ae4b654bcba8aa6e8724adfd19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_79a6ff418b984a8888a49c9f5addebd0",
              "IPY_MODEL_3e1f5490f43f451a910fdb23ba209814",
              "IPY_MODEL_fa7bd41fd6774a988dd1686e111fa244"
            ],
            "layout": "IPY_MODEL_b05d2235cd5d4ef78f3fbc2848696c4c"
          }
        },
        "79a6ff418b984a8888a49c9f5addebd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c8cbbe88f67408dbec1afb227e6401e",
            "placeholder": "​",
            "style": "IPY_MODEL_a546b3c7996845aaa74f66133f5a142a",
            "value": "Generating train split: "
          }
        },
        "3e1f5490f43f451a910fdb23ba209814": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a907399bb2d146d0b5901914fbb4e105",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6a2b9a075d4946499657cfabdc844993",
            "value": 1
          }
        },
        "fa7bd41fd6774a988dd1686e111fa244": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9841b682cf5f47818746326eef77dc8c",
            "placeholder": "​",
            "style": "IPY_MODEL_86a7a0062a3d4b15819f7e4d593824be",
            "value": " 203/0 [00:00&lt;00:00, 2753.24 examples/s]"
          }
        },
        "b05d2235cd5d4ef78f3fbc2848696c4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c8cbbe88f67408dbec1afb227e6401e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a546b3c7996845aaa74f66133f5a142a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a907399bb2d146d0b5901914fbb4e105": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "6a2b9a075d4946499657cfabdc844993": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9841b682cf5f47818746326eef77dc8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86a7a0062a3d4b15819f7e4d593824be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e77e61c974734639abaa4a323f3d3454": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_54d6445d0ab54317adcc74cf4cefaf7d",
              "IPY_MODEL_5aee7c05acc746fe966f350299d1c5da",
              "IPY_MODEL_5cb3cb061b6f4889b0f2cd0fde1b0fbd"
            ],
            "layout": "IPY_MODEL_ccf904a9370a46aea8842989f75c36d4"
          }
        },
        "54d6445d0ab54317adcc74cf4cefaf7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5007086d59e4b3ab0dbc085b14c27c0",
            "placeholder": "​",
            "style": "IPY_MODEL_a906dfd77a174e4ebe758ca96ff0fc16",
            "value": "Map: 100%"
          }
        },
        "5aee7c05acc746fe966f350299d1c5da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62ebded176d742a1977b14aa54baef57",
            "max": 203,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ce109a82f3c74648914b28afc5840e20",
            "value": 203
          }
        },
        "5cb3cb061b6f4889b0f2cd0fde1b0fbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_292ff922522e43a88c8309d35354464e",
            "placeholder": "​",
            "style": "IPY_MODEL_73ead991522740389e7e0a2a422e0a88",
            "value": " 203/203 [00:00&lt;00:00, 3616.49 examples/s]"
          }
        },
        "ccf904a9370a46aea8842989f75c36d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5007086d59e4b3ab0dbc085b14c27c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a906dfd77a174e4ebe758ca96ff0fc16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "62ebded176d742a1977b14aa54baef57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce109a82f3c74648914b28afc5840e20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "292ff922522e43a88c8309d35354464e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73ead991522740389e7e0a2a422e0a88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}